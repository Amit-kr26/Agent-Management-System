{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] =''\n",
    "os.environ[\"TAVILY_API_KEY\"] =''\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "tools=[TavilySearchResults(max_results=1)]\n",
    "prompt=hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm=ChatOpenAI(model='gpt-3.5-turbo',streaming = True)\n",
    "agent_runnable=create_openai_functions_agent(prompt, llm, tools)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Annotated, Union\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: List[BaseMessage]\n",
    "    agent_outcome : Union[AgentAction, AgentFinish,None]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction,str]],operator.add]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentFinish\n",
    "from langchain.prebuilt.tool_executor import ToolExecutor\n",
    "\n",
    "tool_executor=ToolExecutor(tools)\n",
    "def run_agent(data):\n",
    "    agent_outcome=agent_runnable.invoke(data)\n",
    "    return {\"agent_outcome\":agent_outcome}\n",
    "\n",
    "def execute_tool(data):\n",
    "    agent_action=data['agent_outcome']\n",
    "    output=tool_executor.invoke(agent_action)\n",
    "    return {\"intermedikate_steps\":[(agent_action,str(output))]}\n",
    "\n",
    "def should_continue(data):\n",
    "    if isinstance(data['agent_outcome'],AgentFinish):\n",
    "        return \"end\"\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow=StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\",run_agent)\n",
    "workflow.add_node(\"action\",execute_tools)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")   \n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue\n",
    "    {\n",
    "        \"continue\":\"action\",\n",
    "        \"end\":END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge('action','agent')\n",
    "app=workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs={\"input\":\"what is weather ins sf\",\"chat_history\":[]}\n",
    "for s in app.stream(inputs):\n",
    "    print(list(s.values())[0])\n",
    "    print(\"----\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], \"The messages in the conversation\"]\n",
    "    next: str\n",
    "\n",
    "# Define the tools\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: Let's approach this step by step:\n",
    "\n",
    "{agent_scratchpad}\"\"\")\n",
    "\n",
    "# Initialize the Gemma model\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/gemma-2b-it\", \n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 1024},\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "def run_agent(state):\n",
    "    messages = state['messages']\n",
    "    result = agent_executor.invoke({\"input\": messages[-1].content})\n",
    "    return {\n",
    "        \"messages\": messages + [HumanMessage(content=result[\"output\"])],\n",
    "        \"next\": \"action\" if result[\"intermediate_steps\"] else \"end\"\n",
    "    }\n",
    "\n",
    "def execute_tools(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # Assuming the last message contains the tool execution details\n",
    "    # You might need to adjust this based on your actual message structure\n",
    "    tool_name = last_message.additional_kwargs.get(\"tool\")\n",
    "    tool_input = last_message.additional_kwargs.get(\"tool_input\")\n",
    "    if tool_name and tool_input:\n",
    "        tool = next((t for t in tools if t.name == tool_name), None)\n",
    "        if tool:\n",
    "            observation = tool.invoke(tool_input)\n",
    "            return {\"messages\": messages + [HumanMessage(content=str(observation))], \"next\": \"agent\"}\n",
    "    return {\"messages\": messages, \"next\": \"agent\"}\n",
    "\n",
    "def should_continue(state):\n",
    "    return state[\"next\"]\n",
    "\n",
    "# Create the workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.add_node(\"action\", execute_tools)\n",
    "workflow.set_entry_point(\"agent\")   \n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge('action', 'agent')\n",
    "app = workflow.compile()\n",
    "\n",
    "# Now you can use the app\n",
    "inputs = {\"input\": \"What is the weather in SF?\", \"messages\": [HumanMessage(content=\"What is the weather in SF?\")], \"next\": \"agent\"}\n",
    "for s in app.stream(inputs):\n",
    "    print(list(s.values())[0])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import TypedDict, Annotated, Sequence, List, Union\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "import operator\n",
    "\n",
    "# Define the state dictionary for the agent\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: List[BaseMessage]\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    intermediate_steps: Annotated[List[tuple[AgentAction, str]], operator.add]\n",
    "\n",
    "# Define the tools\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: Let's approach this step by step:\n",
    "\n",
    "{agent_scratchpad}\"\"\")\n",
    "\n",
    "# Initialize the Gemma model\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/gemma-2b-it\", \n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 1024},\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "def run_agent(state: AgentState) -> AgentState:\n",
    "    messages = state['chat_history']\n",
    "    result = agent_executor.invoke({\"input\": messages[-1].content})\n",
    "    return {\n",
    "        \"input\": state['input'],\n",
    "        \"chat_history\": messages + [HumanMessage(content=result[\"output\"])],\n",
    "        \"agent_outcome\": result.get('agent_outcome', None),\n",
    "        \"intermediate_steps\": result.get('intermediate_steps', [])\n",
    "    }\n",
    "\n",
    "def execute_tools(state: AgentState) -> AgentState:\n",
    "    messages = state['chat_history']\n",
    "    last_message = messages[-1]\n",
    "    # Assuming the last message contains the tool execution details\n",
    "    tool_name = last_message.additional_kwargs.get(\"tool\")\n",
    "    tool_input = last_message.additional_kwargs.get(\"tool_input\")\n",
    "    if tool_name and tool_input:\n",
    "        tool = next((t for t in tools if t.name == tool_name), None)\n",
    "        if tool:\n",
    "            observation = tool.invoke(tool_input)\n",
    "            return {\n",
    "                \"input\": state['input'],\n",
    "                \"chat_history\": messages + [HumanMessage(content=str(observation))],\n",
    "                \"agent_outcome\": None,\n",
    "                \"intermediate_steps\": []\n",
    "            }\n",
    "    return state\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    return state.get(\"agent_outcome\", None) and \"continue\" or \"end\"\n",
    "\n",
    "# Create the workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.add_node(\"action\", execute_tools)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge('action', 'agent')\n",
    "app = workflow.compile()\n",
    "\n",
    "# Now you can use the app\n",
    "inputs = {\"input\": \"What is the weather in SF?\", \"chat_history\": [HumanMessage(content=\"What is the weather in SF?\")], \"agent_outcome\": None, \"intermediate_steps\": []}\n",
    "for s in app.stream(inputs):\n",
    "    print(list(s.values())[0])\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "import os\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "os.environ[\"TAVILY_API_KEY\"] = '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'\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]='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'\n",
    "#os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]='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'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = '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'\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], \"The messages in the conversation\"]\n",
    "    next: str\n",
    "\n",
    "# Define the tools\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: Let's approach this step by step:\n",
    "\n",
    "{agent_scratchpad}\"\"\")\n",
    "\n",
    "# Initialize the Gemma model\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2b-it\",\n",
    "    temperature=0.7,        # Pass directly here\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "def run_agent(state):\n",
    "    messages = state['messages']\n",
    "    result = agent_executor.invoke({\"input\": messages[-1].content})\n",
    "    return {\n",
    "        \"messages\": messages + [HumanMessage(content=result[\"output\"])],\n",
    "        \"next\": \"action\" if result[\"intermediate_steps\"] else \"end\"\n",
    "    }\n",
    "\n",
    "def execute_tools(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # Assuming the last message contains the tool execution details\n",
    "    # You might need to adjust this based on your actual message structure\n",
    "    tool_name = last_message.additional_kwargs.get(\"tool\")\n",
    "    tool_input = last_message.additional_kwargs.get(\"tool_input\")\n",
    "    if tool_name and tool_input:\n",
    "        tool = next((t for t in tools if t.name == tool_name), None)\n",
    "        if tool:\n",
    "            observation = tool.invoke(tool_input)\n",
    "            return {\"messages\": messages + [HumanMessage(content=str(observation))], \"next\": \"agent\"}\n",
    "    return {\"messages\": messages, \"next\": \"agent\"}\n",
    "\n",
    "def should_continue(state):\n",
    "    return state[\"next\"]\n",
    "\n",
    "# Create the workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.add_node(\"action\", execute_tools)\n",
    "workflow.set_entry_point(\"agent\")   \n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge('action', 'agent')\n",
    "app = workflow.compile()\n",
    "\n",
    "# Now you can use the app\n",
    "inputs = {\"input\": \"What is the weather in SF?\", \"messages\": [HumanMessage(content=\"What is the weather in SF?\")], \"next\": \"agent\"}\n",
    "for s in app.stream(inputs):\n",
    "    print(list(s.values())[0])\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
